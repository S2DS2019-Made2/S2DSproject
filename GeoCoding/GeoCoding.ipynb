{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Geocoding, Self-contained example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This script takes in google analytics city name and region columns, cleans them, and feeds it into several API's to geocode (API-keys need to be provided in the dummy_key.py file). \n",
    "\n",
    "The end-goals here is to create a clean, geocoded file of all the cities that appear in the set of google analytics data fed into it. This could then be used to merge with the original or new data to add socio-economical data that is geographically granular.\n",
    "\n",
    "We also show how one can merge with Eurostat NUTS codes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load packages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Needed packages: \n",
    "\n",
    "- geopy\n",
    "- pandas\n",
    "- matplotlib\n",
    "- numpy\n",
    "- basemap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.basemap import Basemapimport\n",
    "import pandas as pd\n",
    "\n",
    "# import the geocoding services you'd like to try\n",
    "from geopy.geocoders import ArcGIS, Bing, Nominatim, OpenCage, GoogleV3, OpenMapQuest\n",
    "from time import sleep\n",
    "from geopy.extra.rate_limiter import RateLimiter\n",
    "import csv\n",
    "import dummy_keys #change this with the file with your own dummy keys inserted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first load in an example file, where we extracted the city name and region name from a real google analytics dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('example_google_analytics_city_region_file.csv', index_col = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then take a list of all cities/regions that can appear in the google analytics dataset. This file, and newer versions of it can be found here: *https://developers.google.com/analytics/devguides/collection/protocol/v1/geoid*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "google_cities = pd.read_csv('geotargets_google.csv', index_col=0)\n",
    "google_cities = google_cities[['Name','Canonical Name','Country Code','Target Type','Status']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Note: here it might be useful to subselect the google_cities with target type 'City', however, we also found cantons and other types appearing in the raw df data so I continue with all of them. The main downside of this is that certain Names might appear double (for instance one for the Paris as a city and one for region). However, since the canonical name for these different types will still be paris, the API **should** still find the right location for these names.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "google_cities.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use this file to reconstruct the canonical name, which we will feed into the geocoding, from just the city and region names that we got from our google analytics file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Construct country and region names in from canonical name in google list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "google_cities['Country Name'] = google_cities['Canonical Name'].str.split(',').str[-1]\n",
    "google_cities['Region Name'] = google_cities['Canonical Name'].str.split(',').str[-2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now we can find the overlapping cities between those in df and those in google_cities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_cities = google_cities.merge(df, on = ['Region Name','Name'], how = 'right')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(merge_cities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_cities.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that this first pass still left many regions in our data unmatched with their google counterpart. We now try to fix these issues by cleaning the region names in our data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fix regions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We initialize a new 'Clean Region Name' column which we will utilize to try and clean the original region names that were hindering a smooth merge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_cities['Clean Region Name'] = np.nan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fix regions in England"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start with Englang and more specifically with what seem to be TV Regions in the google data, which are fictious regions that for some reason show up in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "google_cities[google_cities['Target Type']=='TV Region'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regions_uk=['South East',\n",
    "'North West',\n",
    "'London',\n",
    "'East of England',\n",
    "'West Midlands',\n",
    "'Midlands',\n",
    "'South West',\n",
    "'Yorkshire',\n",
    "'East Midlands',\n",
    "'North East']\n",
    "merge_cities.loc[merge_cities['Region Name'].isin(regions_uk), 'Country Name'] = 'United Kingdom'\n",
    "merge_cities.loc[merge_cities['Region Name'].isin(regions_uk), 'Country Code'] = 'GB'\n",
    "merge_cities.loc[merge_cities['Region Name'].isin(regions_uk), 'Clean Region Name'] = merge_cities['Region Name'] + ',England'\n",
    "\n",
    "#TV regions\n",
    "tv_regions = google_cities[google_cities['Target Type']=='TV Region']\n",
    "tv_loc = merge_cities['Region Name'].isin(list(tv_regions.Name))\n",
    "tv_loc_dict = tv_regions[['Name','Region Name']].set_index('Name').to_dict()['Region Name']\n",
    "\n",
    "merge_cities.loc[tv_loc, 'Country Name'] = 'United Kingdom'\n",
    "merge_cities.loc[tv_loc, 'Country Code'] = 'GB'\n",
    "merge_cities.loc[tv_loc, 'Target Type'] = 'TV Region'\n",
    "for i in tv_loc_dict.keys():\n",
    "    merge_cities.loc[((tv_loc)&(merge_cities['Region Name'] == i)),'Clean Region Name'] = tv_loc_dict[i]\n",
    "\n",
    "merge_cities.loc[merge_cities['Region Name'].str.contains(\" \\(exc. Channel Islands\\)\"),\"Clean Region Name\"]= merge_cities[\"Region Name\"].str.replace(\" \\(exc. Channel Islands\\)\", \"\", case = True)\n",
    "merge_cities.loc[merge_cities['Region Name'].str.contains(\"HTV\"),\"Clean Region Name\"]= merge_cities[\"Region Name\"].str.replace(\"HTV \", \"\", case = True)\n",
    "\n",
    "merge_cities.loc[((merge_cities['Country Code'] == 'GB') & (merge_cities['Canonical Name'].isnull())), 'Canonical Name'] = merge_cities.Name + ',' + merge_cities['Clean Region Name'] + ',' + merge_cities['Country Name']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fix american state appendices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we move on to the american states. It seems that in our cities, sometime the abbreviation for the state is added, which hinders the merge. Below we show an example for Chicago, where the abbreviation IL for Illinois is added."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_cities[merge_cities['Region Name'].str.contains('IL')].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "US_state_full = {'AL': 'Alabama',\n",
    " 'AK': 'Alaska',\n",
    " 'AZ': 'Arizona',\n",
    " 'AR': 'Arkansas',\n",
    " 'CA': 'California',\n",
    " 'CO': 'Colorado',\n",
    " 'CT': 'Connecticut',\n",
    " 'DE': 'Delaware',\n",
    " 'DC': 'District of Columbia',\n",
    " 'FL': 'Florida',\n",
    " 'GA': 'Georgia',\n",
    " 'HI': 'Hawaii',\n",
    " 'ID': 'Idaho',\n",
    " 'IL': 'Illinois',\n",
    " 'IN': 'Indiana',\n",
    " 'IA': 'Iowa',\n",
    " 'KS': 'Kansas',\n",
    " 'KY': 'Kentucky',\n",
    " 'LA': 'Louisiana',\n",
    " 'ME': 'Maine',\n",
    " 'MD': 'Maryland',\n",
    " 'MA': 'Massachusetts',\n",
    " 'MI': 'Michigan',\n",
    " 'MN': 'Minnesota',\n",
    " 'MS': 'Mississippi',\n",
    " 'MO': 'Missouri',\n",
    " 'MT': 'Montana',\n",
    " 'NE': 'Nebraska',\n",
    " 'NV': 'Nevada',\n",
    " 'NH': 'New Hampshire',\n",
    " 'NJ': 'New Jersey',\n",
    " 'NM': 'New Mexico',\n",
    " 'NY': 'New York',\n",
    " 'NC': 'North Carolina',\n",
    " 'ND': 'North Dakota',\n",
    " 'MP': 'Northern Mariana Islands',\n",
    " 'OH': 'Ohio',\n",
    " 'OK': 'Oklahoma',\n",
    " 'OR': 'Oregon',\n",
    " 'PW': 'Palau',\n",
    " 'PA': 'Pennsylvania',\n",
    " 'PR': 'Puerto Rico',\n",
    " 'RI': 'Rhode Island',\n",
    " 'SC': 'South Carolina',\n",
    " 'SD': 'South Dakota',\n",
    " 'TN': 'Tennessee',\n",
    " 'TX': 'Texas',\n",
    " 'UT': 'Utah',\n",
    " 'VT': 'Vermont',\n",
    " 'VI': 'Virgin Islands',\n",
    " 'VA': 'Virginia',\n",
    " 'WA': 'Washington',\n",
    " 'WV': 'West Virginia',\n",
    " 'WI': 'Wisconsin',\n",
    " 'WY': 'Wyoming'}\n",
    "\n",
    "american_States_space_sep = ((merge_cities['Canonical Name'].isnull())&(merge_cities['Region Name'].str.split(' ').str[-1].str.isupper())&(merge_cities['Region Name'].str.split(' ').str[-1].str.isalpha())& (merge_cities['Region Name'].str[-2:].str.isalpha()))\n",
    "\n",
    "merge_cities.loc[american_States_space_sep,'Country Name'] = 'United States'\n",
    "merge_cities.loc[american_States_space_sep,'Country Code'] = 'US'\n",
    "merge_cities.loc[american_States_space_sep,'Clean Region Name'] = merge_cities['Region Name'].str[-2:]\n",
    "\n",
    "american_States_comma_sep = ((merge_cities['Canonical Name'].isnull())&(merge_cities['Region Name'].str.split(',').str[-1].str.isupper())&(merge_cities['Region Name'].str.split(',').str[-1].str.isalpha())& (merge_cities['Region Name'].str[-2:].str.isalpha()))\n",
    "\n",
    "merge_cities.loc[american_States_comma_sep,'Country Name'] = 'United States'\n",
    "merge_cities.loc[american_States_comma_sep,'Country Code'] = 'US'\n",
    "merge_cities.loc[american_States_comma_sep,'Clean Region Name'] = merge_cities['Region Name'].str[-2:]\n",
    "\n",
    "for i in US_state_full.keys():\n",
    "    merge_cities.loc[merge_cities['Country Code']=='US','Clean Region Name'] = merge_cities['Clean Region Name'].str.replace(i,US_state_full[i])\n",
    "\n",
    "merge_cities.loc[((merge_cities['Country Code']=='US') & (merge_cities['Canonical Name'].isnull())), 'Canonical Name'] = merge_cities.Name + ',' + merge_cities['Clean Region Name'] + ',' + merge_cities['Country Name']\n",
    "\n",
    "dc = merge_cities['Region Name'] == 'Washington DC (Hagerstown MD)'\n",
    "merge_cities.loc[dc,'Country Name'] = 'United States'\n",
    "merge_cities.loc[dc,'Country Code'] = 'US'\n",
    "merge_cities.loc[dc,'Clean Region Name'] = 'Washington DC'\n",
    "merge_cities.loc[dc,'Canonical Name'] = merge_cities.Name + ',' + merge_cities['Clean Region Name'] + ',' + merge_cities['Country Name']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_cities[merge_cities['Region Name'].str.contains('IL')].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cantons in Switserland"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next peculiarity we tackle are the Cantons in Switserland, where for some reason we have a quite useless prefix 'Canton of'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_cities[(merge_cities['Canonical Name'].isnull())&(merge_cities['Region Name'].str.contains('Canton of'))].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cantons = (merge_cities['Canonical Name'].isnull())&(merge_cities['Region Name'].str.contains('Canton of'))\n",
    "merge_cities.loc[cantons,'Country Name'] = 'Switzerland'\n",
    "merge_cities.loc[cantons,'Country Code'] = 'CH'\n",
    "merge_cities.loc[cantons,'Canonical Name'] = merge_cities.Name + ',' + merge_cities['Region Name'] + ',' + merge_cities['Country Name']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_cities[merge_cities['Region Name'].str.contains('Neuchatel')].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fix JP_ prefix for Japan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, I found that for Japan, there is a JP_ prefix. So let's fix this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_cities[merge_cities['Region Name'].str.contains('JP_')].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "japan = (merge_cities['Canonical Name'].isnull())&(merge_cities['Region Name'].str.contains('JP_'))\n",
    "merge_cities.loc[japan,'Country Name'] = 'Japan'\n",
    "merge_cities.loc[japan,'Country Code'] = 'JP'\n",
    "merge_cities.loc[japan&~(merge_cities['Region Name'].str.split('_').str[-1] == 'OTHER'),'Clean Region Name'] = merge_cities['Region Name'].str.split('_').str[-1]\n",
    "merge_cities.loc[japan,'Canonical Name'] = merge_cities.Name + ',' + merge_cities['Clean Region Name'] + ',' + merge_cities['Country Name']\n",
    "merge_cities.loc[japan&(merge_cities['Region Name'].str.split('_').str[-1] == 'OTHER'),'Canonical Name'] = merge_cities.Name + ',' + merge_cities['Country Name']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "merge_cities[merge_cities['Region Name'].str.contains('JP_')].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For those cities not found merge name and region name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the remaining city, we just have to kind of hope that the canonical name we compile from available information is precise enough so that the API's can give us their best guess of where these locations are."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "overlap_places = merge_cities[~merge_cities['Canonical Name'].isnull()]\n",
    "non_overlap_places = merge_cities[merge_cities['Canonical Name'].isnull()]\n",
    "\n",
    "for i in list(non_overlap_places.Name):\n",
    "    if i in list(overlap_places.Name):\n",
    "        merge_cities.loc[((merge_cities['Name']==i)&(merge_cities['Canonical Name'].isnull())),'Country Name'] = overlap_places.loc[overlap_places.Name == i,'Country Name'].values[0]\n",
    "        merge_cities.loc[((merge_cities['Name']==i)&(merge_cities['Canonical Name'].isnull())),'Country Code'] = overlap_places.loc[overlap_places.Name == i,'Country Code'].values[0]\n",
    "        merge_cities.loc[((merge_cities['Name']==i)&(merge_cities['Canonical Name'].isnull())&(merge_cities['Clean Region Name'].isnull())),'Canonical Name'] = merge_cities.Name + ',' + merge_cities['Region Name'] + ',' + merge_cities['Country Name']\n",
    "        merge_cities.loc[((merge_cities['Name']==i)&(merge_cities['Canonical Name'].isnull())&~(merge_cities['Clean Region Name'].isnull())),'Canonical Name'] = merge_cities.Name + ',' + merge_cities['Clean Region Name'] + ',' + merge_cities['Country Name']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "overlap_places = merge_cities[~merge_cities['Canonical Name'].isnull()]\n",
    "non_overlap_places = merge_cities[merge_cities['Canonical Name'].isnull()]\n",
    "\n",
    "non_overlap_places['Canonical Name'] = non_overlap_places.Name.astype(str) + ',' + non_overlap_places['Region Name'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "overlap_places.to_csv('overlap_regions.csv', encoding='utf-8', index=False)\n",
    "non_overlap_places.to_csv('non_overlap_regions.csv', encoding='utf-8', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we write out two files, one for those we are pretty sure our description is accurate, and for those where we are less sure. I do this because for instance the Google API can handle such uncertain name a lot better than other API's. But since the free version has a limit on the amount of requests, we would request these first before running out of free requests. The more certain ones can then be requested from several API's."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Geocoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initializing API's"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here it is *Key* to have a keys.py file in the folder that contains your personal API-keys."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "geocoded = \"geocoded/\"\n",
    "gc_directory = os.path.dirname(geocoded)\n",
    "if not os.path.exists(gc_directory):\n",
    "    os.makedirs(gc_directory)\n",
    "\n",
    "rv_geocoded = \"reverse_geocoded/\"\n",
    "rv_directory = os.path.dirname(rv_geocoded)\n",
    "if not os.path.exists(rv_directory):\n",
    "    os.makedirs(rv_directory)\n",
    "\n",
    "in_file = str('non_overlap_regions.csv')\n",
    "#add gc_ to geocoded file name\n",
    "out_file = str('gc_' + in_file)\n",
    "timeout = 100\n",
    "\n",
    "print('creating geocoding objects.')\n",
    "\n",
    "arcgis = ArcGIS(timeout=timeout)\n",
    "bing = Bing(api_key=keys.bing_api,timeout=timeout)\n",
    "nominatim = Nominatim(user_agent='application_n', timeout=timeout)\n",
    "opencage = OpenCage(api_key=keys.oc_api,timeout=timeout)\n",
    "googlev3 = GoogleV3(api_key=keys.g3_api, domain='maps.googleapis.com', timeout=timeout)\n",
    "openmapquest = OpenMapQuest(api_key=keys.omq_api, timeout=timeout)\n",
    "\n",
    "# choose and order your preference for geocoders here\n",
    "geocoders = [googlev3,nominatim, bing, openmapquest, opencage, arcgis]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Geocoding function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (Expects a 'Canonical Name' column as google analytics uses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gc(row):\n",
    "    add_concat = row['Canonical Name']\n",
    "    for gcoder in geocoders:\n",
    "        location = gcoder.geocode(add_concat)\n",
    "        if location != None:\n",
    "            print(f'geocoded record {add_concat}')\n",
    "            located = pd.Series({\n",
    "                'lat': location.latitude,\n",
    "                'lng': location.longitude\n",
    "            })\n",
    "        else:\n",
    "            print(f'failed to geolocate record {add_concat}')\n",
    "            located = pd.Series({\n",
    "                'lat': 'null',\n",
    "                'lng': 'null'\n",
    "            })\n",
    "        return located"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reverse Geocoding function nominatim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Expects a lat and lng column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reverse_gc(row):\n",
    "    sleep(0.8) # to prevent overloading the API and getting too many request errors\n",
    "    add_concat = str(row['lat']) + ',' + str(row['lng'])\n",
    "    gcoder = True\n",
    "    #for gcoder in geocoders:\n",
    "    reverse = RateLimiter(nominatim.reverse, min_delay_seconds=0.7)\n",
    "    location = reverse(add_concat, language='en', exactly_one = True)\n",
    "    if location != None:\n",
    "        print(f'reverse geocoded record {add_concat}')\n",
    "        city = 'null'\n",
    "        if 'city' in location.raw['address'].keys():\n",
    "            city = location.raw['address']['city']\n",
    "        elif 'town' in location.raw['address'].keys():\n",
    "            city = location.raw['address']['town']\n",
    "        elif 'village' in location.raw['address'].keys():\n",
    "            city = location.raw['address']['village']\n",
    "        elif 'suburb' in location.raw['address'].keys():\n",
    "            city = location.raw['address']['suburb']\n",
    "        elif 'hamlet' in location.raw['address'].keys():\n",
    "            city = location.raw['address']['hamlet']\n",
    "                    \n",
    "        county = 'null'\n",
    "        if 'county' in location.raw['address'].keys():\n",
    "            county = location.raw['address']['county']\n",
    "\n",
    "        state = 'null'\n",
    "        if 'state' in location.raw['address'].keys():\n",
    "            state = location.raw['address']['state']\n",
    "\n",
    "        region = 'null'\n",
    "        if 'region' in location.raw['address'].keys():\n",
    "            region = location.raw['address']['region']\n",
    "\n",
    "        postcode = 'null'\n",
    "        if 'postcode' in location.raw['address'].keys():\n",
    "            postcode = location.raw['address']['postcode']\n",
    "\n",
    "        country = 'null'\n",
    "        if 'country' in location.raw['address'].keys():\n",
    "            country = location.raw['address']['country']\n",
    "            \n",
    "        country_code = 'null'\n",
    "        if 'country_code' in location.raw['address'].keys():\n",
    "            country_code = location.raw['address']['country_code']\n",
    "     \n",
    "\n",
    "        located = pd.Series({\n",
    "            'city_reverse': city,\n",
    "            'county_reverse': county,\n",
    "            'state_reverse': state,\n",
    "            'region_reverse': region,\n",
    "            'postcode_reverse': postcode,\n",
    "            'country_reverse': country,\n",
    "            'country_code_reverse': country_code\n",
    "        })\n",
    "\n",
    "    else:\n",
    "        print(f'failed to reverse geolocate record {add_concat}')\n",
    "        located = pd.Series({\n",
    "            'city_reverse': 'null',\n",
    "            'county_reverse': 'null',\n",
    "            'state_reverse': 'null',\n",
    "            'region_reverse': 'null',\n",
    "            'postcode_reverse': 'null',\n",
    "            'country_reverse': 'null',\n",
    "            'country_code_reverse': 'null'\n",
    "        })\n",
    "    return located"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Geocoding csv file in chunks to avoid data loss if connection failure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('opening input.')\n",
    "reader = pd.read_csv(in_file, header=0)\n",
    "nr_chunks = 0\n",
    "#if your file is too large we split it up into files of around 500 entries to avoid too much loss of progress when errors are thrown\n",
    "if len(reader) > 800:\n",
    "    nr_chunks = int(np.floor(len(reader)/500))\n",
    "    chunks = np.array_split(reader, nr_chunks)\n",
    "    print(f'Splitted data into {nr_chunks} chunks.')\n",
    "    i = 1\n",
    "    for chunk in chunks:\n",
    "        print(f'starting geocoding on chunk {i}')\n",
    "        try:\n",
    "            #this again uses the same order as your original list of API's, here the google-API is tried first\n",
    "            chunk = chunk.merge(chunk.apply(lambda add: gc(add), axis=1), left_index=True, right_index=True)\n",
    "        except Exception as e:\n",
    "            print(f'Caught exception, stopping geocoding. See geocode_error.txt for more info.')\n",
    "            file = open('geocode_error.txt','w') \n",
    "            file.write(f\"Error occured at chunk {i}\") \n",
    "            file.write(f\"The error-code was:'{str(e)}'\") \n",
    "            file.close()\n",
    "            \n",
    "        out_file_chunk = str(f'chunk_{i}_of_{nr_chunks}_' + out_file)\n",
    "        print(f'writing chunk {i} to {out_file_chunk}.')\n",
    "        chunk.to_csv(geocoded+out_file_chunk, encoding='utf-8', index=False)\n",
    "        i+=1\n",
    "    print('done.')\n",
    "\n",
    "else:\n",
    "    print('geocoding addresses.')\n",
    "    reader = reader.merge(reader.apply(lambda add: gc(add), axis=1), left_index=True, right_index=True)\n",
    "    print(f'writing to {out_file}.')\n",
    "    reader.to_csv(geocoded+out_file, encoding='utf-8', index=False)\n",
    "    print('done.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# From the geocoded file we can now revert back to get the postcodes, which is more useful since government data is often on this level."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inverting from coordinates to places and POSTCODES for all the chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from time import sleep\n",
    "from geopy.extra.rate_limiter import RateLimiter\n",
    "\n",
    "if nr_chunks>0:\n",
    "    for i in range(1,nr_chunks+1):\n",
    "        in_file_chunk = str(f'chunk_{i}_of_{nr_chunks}_' + out_file)\n",
    "        print(f'opening chunk {i}.')\n",
    "        reader = pd.read_csv(in_file_chunk, header=0)\n",
    "        print('reverse geocoding addresses.')\n",
    "        reader = reader.merge(reader.apply(lambda add: reverse_gc(add), axis=1), left_index=True, right_index=True)\n",
    "        out_file_chunk = str('rv_' + in_file_chunk)\n",
    "        print(f'writing to {out_file_chunk}.')\n",
    "        reader.to_csv(rv_geocoded+out_file_chunk, encoding='utf-8', index=False)\n",
    "        print('done.')\n",
    "else:\n",
    "    print(f'opening file.')\n",
    "    reader = pd.read_csv(out_file, header=0)\n",
    "    print('reverse geocoding addresses.')\n",
    "    reader = reader.merge(reader.apply(lambda add: reverse_gc(add), axis=1), left_index=True, right_index=True)\n",
    "    out_file_rv = str('rv_' + out_file)\n",
    "    print(f'writing to {out_file_rv}.')\n",
    "    reader.to_csv(rv_geocoded+out_file_rv, encoding='utf-8', index=False)\n",
    "    print('done.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Googlev3 reverse encoding API (Alternative for Nomatim above)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_google_component(location, component_type, long_name=True):\n",
    "    for component in location.raw['address_components']:\n",
    "        if component_type in component['types']:\n",
    "            if long_name:\n",
    "                return component['long_name']\n",
    "            else:\n",
    "                return component['short_name']\n",
    "\n",
    "\n",
    "def google_reverse_gc(row):\n",
    "    add_concat = str(row['lat']) + ',' + str(row['lng'])\n",
    "    gcoder = True\n",
    "    reverse = RateLimiter(googlev3.reverse, min_delay_seconds=0.7)\n",
    "    location = reverse(add_concat, language='en', exactly_one = True)\n",
    "    if location != None:\n",
    "        print(f'reverse geocoded record {add_concat}')\n",
    "        city = get_google_component(location, 'postal_town')\n",
    "        county = get_google_component(location, 'administrative_area_level_2')\n",
    "        region = get_google_component(location, 'administrative_area_level_1')\n",
    "        post_code = get_google_component(location, 'postal_code')\n",
    "        country = get_google_component(location, 'country')\n",
    "        country_code = get_google_component(location, 'country', long_name=False)\n",
    "\n",
    "        located = pd.Series({\n",
    "            'city_reverse': city,\n",
    "            'county_reverse': county,\n",
    "            'region_reverse': region,\n",
    "            'postcode_reverse': post_code,\n",
    "            'country_reverse': country,\n",
    "            'country_code_reverse': country_code\n",
    "        })\n",
    "\n",
    "    else:\n",
    "        print(f'failed to reverse geolocate record {add_concat}')\n",
    "        located = pd.Series({\n",
    "            'city_reverse': 'null',\n",
    "            'county_reverse': 'null',\n",
    "            'region_reverse': 'null',\n",
    "            'postcode_reverse': 'null',\n",
    "            'country_reverse': 'null',\n",
    "            'country_code_reverse': 'null'\n",
    "        })\n",
    "    return located"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from time import sleep\n",
    "from geopy.extra.rate_limiter import RateLimiter\n",
    "\n",
    "if nr_chunks>0:\n",
    "    for i in range(1,nr_chunks+1):\n",
    "        in_file_chunk = str(f'chunk_{i}_of_{nr_chunks}_' + out_file)\n",
    "        print(f'opening chunk {i}.')\n",
    "        reader = pd.read_csv(in_file_chunk, header=0)\n",
    "        print('reverse geocoding addresses.')\n",
    "        reader = reader.merge(reader.apply(lambda add: google_reverse_gc(add), axis=1), left_index=True, right_index=True)\n",
    "        out_file_chunk = str('google_rv_' + in_file_chunk)\n",
    "        print(f'writing to {out_file_chunk}.')\n",
    "        reader.to_csv(rv_geocoded+out_file_chunk, encoding='utf-8', index=False)\n",
    "        print('done.')\n",
    "else:\n",
    "    print(f'opening file.')\n",
    "    reader = pd.read_csv(out_file, header=0)\n",
    "    print('reverse geocoding addresses.')\n",
    "    reader = reader.merge(reader.apply(lambda add: google_reverse_gc(add), axis=1), left_index=True, right_index=True)\n",
    "    out_file_rv = str('google_rv_' + out_file)\n",
    "    print(f'writing to {out_file_rv}.')\n",
    "    reader.to_csv(rv_geocoded+out_file_rv, encoding='utf-8', index=False)\n",
    "    print('done.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Connecting postal codes to NUTS codes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge all reverse_geocoded_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "\n",
    "path = 'reverse_geocoded/' # use your path\n",
    "all_files = glob.glob(path + \"/*.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "li = []\n",
    "ii=0\n",
    "if nr_chunks>0:\n",
    "    for filename in all_files:\n",
    "        print(filename)\n",
    "        df = pd.read_csv(filename,header = 0, index_col=0)\n",
    "        print(len(df))\n",
    "        ii+=len(df)\n",
    "        li.append(df)\n",
    "\n",
    "    frame = pd.concat(li, axis=0)\n",
    "    print(ii)\n",
    "    frame.to_csv(path+'all_reverse_geocoded.csv',encoding='utf-8')\n",
    "    all_gc_path = path+'all_reverse_geocoded.csv'\n",
    "else:\n",
    "    for filename in all_files:\n",
    "        if 'google' in filename:\n",
    "            all_gc_path = path+'google_rv_gc_non_overlap_regions.csv'\n",
    "        else:\n",
    "            all_gc_path = path+'rv_gc_non_overlap_regions.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retry unknown postcodes with google api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(all_gc_path,header=0, index_col=None)\n",
    "#df = df.reset_index().drop(columns = ['index'])\n",
    "df.loc[df['Canonical Name'] == 'Oxted,England,United Kingdom', 'lng'] = 0\n",
    "\n",
    "retry = df[df.postcode_reverse.isnull()]\n",
    "df_nn = df[~df.postcode_reverse.isnull()]\n",
    "\n",
    "reader = retry[['Canonical Name',\n",
    " 'Clean Region Name',\n",
    " 'Country Code',\n",
    " 'Country Name',\n",
    " 'Name',\n",
    " 'Region Name',\n",
    " 'Status',\n",
    " 'Target Type',\n",
    " 'lat',\n",
    " 'lng',\n",
    " 'state_reverse']]\n",
    "\n",
    "print('reverse geocoding addresses.')\n",
    "reader = reader.merge(reader.apply(lambda add: google_reverse_gc(add), axis=1), left_index=True, right_index=True)\n",
    "df = pd.concat([df_nn,reader[~reader.postcode_reverse.isnull()]], axis=0)\n",
    "df.to_csv(path+'all_reverse_geocoded_plus_retry.csv',encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point you have a datafile that contains all the city/region combination you fed into the processing together with all the geo-features you need to merge it with statistical region codes/data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's start merging with the NUTS code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'reverse_geocoded/'\n",
    "df = pd.read_csv(path+'all_reverse_geocoded_plus_retry.csv',encoding='utf-8',header=0, index_col=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fix some peculiarities with different country codes between google and nuts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.country_code_reverse = df.country_code_reverse.str.lower()\n",
    "df.loc[df.country_code_reverse=='gb','country_code_reverse'] = 'uk'\n",
    "df.loc[df.country_code_reverse=='gr','country_code_reverse'] = 'el'\n",
    "df.loc[(df.postcode_reverse.str.contains('\\.')),'postcode_reverse'] = df.postcode_reverse.str[:-2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read in the postal codes to nuts codes file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nuts = pd.read_csv('all_nuts_to_postal_country_code.csv',encoding='utf-8',header=0, index_col=0)\n",
    "nuts = nuts.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### perform the first merge try"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_step = df.merge(nuts.dropna(),how='left', on=['postcode_reverse','country_code_reverse'])\n",
    "first_step = first_step[first_step.country_code_reverse.isin(list(nuts.country_code_reverse.unique()))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### checking if we can find nuts code for postcode with same prefix, which is accurate enough for NUTS3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "postcodes = nuts.postcode_reverse\n",
    "to_do = list(first_step[first_step.NUTS3.isnull()].postcode_reverse.unique())\n",
    "for i in to_do:\n",
    "    if ' ' in i.strip():\n",
    "        i_start = i.split(' ')[0]\n",
    "        if sum(nuts.postcode_reverse.str.startswith(i_start)) > 0:\n",
    "            nuts_code = nuts[postcodes.str.startswith(i_start)].NUTS3.values[0]\n",
    "            first_step.loc[first_step.postcode_reverse == i, 'NUTS3'] = nuts_code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nuts_path = \"postcode_to_nuts/\"\n",
    "nuts_directory = os.path.dirname(nuts_path)\n",
    "if not os.path.exists(nuts_directory):\n",
    "    os.makedirs(nuts_directory)\n",
    "first_step.to_csv(nuts_path+'city_to_nuts_iteration1.csv',encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### fixing german starting 0's in postcode, which don't exist in eurostats tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "postcodes = nuts.postcode_reverse\n",
    "to_do_de = list(first_step[(first_step.NUTS3.isnull())&(first_step.country_code_reverse == 'de')].postcode_reverse.unique())\n",
    "for i in to_do_de:\n",
    "    if i.strip().startswith('0'):\n",
    "        i_end = i[1:]\n",
    "        matches = nuts[(nuts.postcode_reverse == i_end)&(nuts.country_code_reverse == 'de')]\n",
    "        if len(matches) > 0:\n",
    "            nuts_code = matches.NUTS3.values[0]\n",
    "            first_step.loc[first_step.postcode_reverse == i, 'NUTS3'] = nuts_code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### doing the same for spanish cities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "postcodes = nuts.postcode_reverse\n",
    "to_do_es = list(first_step[(first_step.NUTS3.isnull())&(first_step.country_code_reverse == 'es')].postcode_reverse.unique())\n",
    "for i in to_do_es:\n",
    "    if i.strip().startswith('0'):\n",
    "        i_end = i[1:]\n",
    "        matches = nuts[(nuts.postcode_reverse == i_end)&(nuts.country_code_reverse == 'es')]\n",
    "        if len(matches) > 0:\n",
    "            nuts_code = matches.NUTS3.values[0]\n",
    "            first_step.loc[first_step.postcode_reverse == i, 'NUTS3'] = nuts_code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Le mÃªme pour la republique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "postcodes = nuts.postcode_reverse\n",
    "to_do_fr = list(first_step[(first_step.NUTS3.isnull())&(first_step.country_code_reverse == 'fr')].postcode_reverse.unique())\n",
    "for i in to_do_fr:\n",
    "    if i.strip().startswith('0'):\n",
    "        i_end = i[1:]\n",
    "        matches = nuts[(nuts.postcode_reverse.str.startswith(i_end))&(nuts.country_code_reverse == 'fr')]\n",
    "        if len(matches) > 0:\n",
    "            nuts_code = matches.NUTS3.values[0]\n",
    "            first_step.loc[first_step.postcode_reverse == i, 'NUTS3'] = nuts_code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### fixing the dutch codes which have street identifiers e.g. 3295BW (the BW is a nuissance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "postcodes = nuts.postcode_reverse\n",
    "to_do_nl = list(first_step[(first_step.NUTS3.isnull())&(first_step.country_code_reverse == 'nl')].postcode_reverse.unique())\n",
    "for i in to_do_nl:\n",
    "    if i[-2:].isalpha():\n",
    "        i_cutted = i[:-2]\n",
    "        matches = nuts[(nuts.postcode_reverse == i_cutted)&(nuts.country_code_reverse == 'nl')]\n",
    "        if len(matches) > 0:\n",
    "            nuts_code = matches.NUTS3.values[0]\n",
    "            first_step.loc[first_step.postcode_reverse == i, 'NUTS3'] = nuts_code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_step.to_csv(nuts_path+'city_to_nuts_up_till_nl.csv',encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### first pass at fixing the english"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "postcodes = nuts.postcode_reverse\n",
    "to_do_uk = list(first_step[(first_step.NUTS3.isnull())&(first_step.country_code_reverse == 'uk')].postcode_reverse.unique())\n",
    "for i in to_do_uk:\n",
    "    i_start = i.split(' ')[0]\n",
    "    matches = nuts[(nuts.postcode_reverse.str.startswith(f'{i_start} '))&(nuts.country_code_reverse == 'uk')]\n",
    "    if len(matches) > 0:\n",
    "        nuts_code = matches.NUTS3.values[0]\n",
    "        first_step.loc[first_step.postcode_reverse == i, 'NUTS3'] = nuts_code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### second pass at fixing the english by seeing if there is a neighbouring number that matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "postcodes = nuts.postcode_reverse\n",
    "to_do_uk = list(first_step[(first_step.NUTS3.isnull())&(first_step.country_code_reverse == 'uk')].postcode_reverse.unique())\n",
    "for i in to_do_uk:\n",
    "    i_start = i.split(' ')[0]\n",
    "    i_start_1 = i_start[:-1]\n",
    "    if i_start[-1].isdigit():\n",
    "        lastdigit = str(int(i_start[-1]) + 1)\n",
    "        i_start = i_start_1 + lastdigit\n",
    "        matches = nuts[(nuts.postcode_reverse.str.startswith(f'{i_start} '))&(nuts.country_code_reverse == 'uk')]\n",
    "        if len(matches) > 0:\n",
    "            nuts_code = matches.NUTS3.values[0]\n",
    "            first_step.loc[first_step.postcode_reverse == i, 'NUTS3'] = nuts_code\n",
    "        else:\n",
    "            i_start = i.split(' ')[0]\n",
    "            i_start_1 = i_start[:-1]\n",
    "            lastdigit = str(int(i_start[-1]) -1)\n",
    "            i_start = i_start_1 + lastdigit\n",
    "            matches = nuts[(nuts.postcode_reverse.str.startswith(f'{i_start} '))&(nuts.country_code_reverse == 'uk')]\n",
    "            if len(matches) > 0:\n",
    "                nuts_code = matches.NUTS3.values[0]\n",
    "                first_step.loc[first_step.postcode_reverse == i, 'NUTS3'] = nuts_code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_step.to_csv(nuts_path+'city_to_nuts_up_till_uk.csv',encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### cutting number of the french postcodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "postcodes = nuts.postcode_reverse\n",
    "to_do_fr = list(first_step[(first_step.NUTS3.isnull())&(first_step.country_code_reverse == 'fr')].postcode_reverse.unique())\n",
    "for i in to_do_fr:\n",
    "    if i[-1:].isdigit():\n",
    "        i_cutted = i[:-1]\n",
    "        matches = nuts[(nuts.postcode_reverse.str.startswith(i_cutted))&(nuts.country_code_reverse == 'fr')]\n",
    "        if len(matches) > 0:\n",
    "            nuts_code = matches.NUTS3.values[0]\n",
    "            first_step.loc[first_step.postcode_reverse == i, 'NUTS3'] = nuts_code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_step.drop_duplicates(subset=['Canonical Name','NUTS3'], keep='first', inplace=True)\n",
    "first_step.to_csv('city_to_nuts_final_clean.csv',encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A bit of cleaning up files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import shutil\n",
    "shutil.rmtree(nuts_path[:-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**This file can now be used to add all the geo-features collected here to your data.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plotting the cities that visited the website"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_step = pd.read_csv('city_to_nuts_final_clean.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lats = list(first_step.lat)\n",
    "lons = list(first_step.lng)\n",
    "\n",
    "fig = plt.gcf()\n",
    "fig.set_size_inches(8, 6.5)\n",
    "\n",
    "m = Basemap(projection='merc', \\\n",
    "            llcrnrlat=-80, urcrnrlat=80, \\\n",
    "            llcrnrlon=-180, urcrnrlon=180, \\\n",
    "            lat_ts=20, \\\n",
    "            resolution='c')\n",
    "\n",
    "m.bluemarble(scale=0.2)   # full scale will be overkill\n",
    "m.drawcoastlines(color='white', linewidth=0.2)  # add coastlines\n",
    "\n",
    "x, y = m(lons, lats)  # transform coordinates\n",
    "plt.scatter(x, y, 10, marker='o', color='Red') \n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
